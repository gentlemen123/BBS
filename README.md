# 求职论坛信息爬虫

## 爬取信息：
- 邮箱 （最重要）
- 电话
- 发帖时间 （次重要）
- 标题

## 要求（按重要性排名）：
1. 邮箱相同，帖子不同，只留发帖时间最新的帖子
2. 一个帖子有多个邮箱，都保留，但是存储时，一行只保留一个邮箱
3. 帖子回复内容不爬


## MongoDB中数据项:
- article_id:字符串   (可选)文章id,全局唯一,多为链接中获取
- article:字符串 招聘信息文章内容
- article_link:字符串 文章链接
- title:字符串 文章标题
- update_time:字符串 最后更新或阅读时间
- publish_time:字符串 招聘信息发表时间
- email:列表 文章内容解析后的邮箱
- telephone:列表 电话
- status: 文章操作状态
    - not_done:文章未抓取
    - not-exit: 文章不存在
    - fetched:文章已抓取,但未解析
    - not-email:文章解析没有邮箱
    - done: 文章已解析,获得邮箱,但未全局除重
    - not-one: 文章除重,属于多余的邮箱
    - one: 文章已除重,邮箱全局唯一,但未输出
    - output:文章已输出!!!!
    
## 新渠道需要更新文件:
1. config.py: 爬虫配置文件.name:数据库名字,输出问价名字/headers:请求头
2. webpage_crawlerpy: 下一页链接获取文件.获取下一页或者最大页数
3. article_link_crawler.py: 每页文章链接标签定位xpath更新,酌情插入元素
4. get_article.py: 文章标签定位xpath更新,酌情插入元素
5. crawler.py: 爬虫主程序.关心上面三个

## 新渠道不需更新文件:
1. article_analysis.py:解析文件,**只为获取**获取邮箱和电话.   只要文件是字符串,就不需更改这一环
2. email_filter.py: 邮件除重文件,邮件是列表形式, 除重条件是时间(**注意时间格式**), 只关注数据库中两个文件邮箱的整体比较,而不关注邮箱列表中的细节比较
3. out_as_file.py: 数据输出文件,只要严格按照文件各个状态来,就完全无需关心,只是输出文件
4. data_storage.py: 数据库连接文件,完全无需关心
5. log.py: 日志文件,关注各个状态数目,完全无需关心

    